---
title: 1、第一周总结-01
tags: 
notebook: 第六个月总结笔记
---


# 第一天
### 获取数据 使用爬虫获取 也有专门的网站收集数据

### 网页的特性
    1、url
    2、用html展示
    3、通过http访问 
### 除了python还有其他语言可以爬虫
    php不能多线程
    java修改太麻烦
    c太难

    python 简单可以调用其他语言的模块
### 实现爬虫的步骤
    1、确定url
    2、发起请求，服务器回应
    3、拿到数据
### 爬虫分类
    普通爬虫 为搜索引擎提供 漫无目的的
    聚焦爬虫：有目的、有针对行的去互联网获取数据
### 语法
    import urllib.request
    url = 'http://www.tuniu.com/theme/qinzi/'
    response = urllib.request.urlopen(url) #请求
### 打开方式
    w 写
    r 读
    a 追加
### 各种模块导入
    urllib  是python自带的模块可以发起请求获取请求
    urllib.request 构建请求,发起请求获取回应
    urllib.parse 处理url的
    urllib.error 处理错误，一般都是try来用的
    ssl 这个是https免证书认证
### urlopen方法
    urllib.request.urlopen() 发起请求， 返回一个对象（起个名字就叫response）
        参数介绍：
            1、url
            2、data
            3、timeout
            4、cafile
            5、capath
            6、context
### response的方法有
    response.decode('utf-8') 转成字符串
    response.encode('utf-8') 转成二进制
    response.getheaders()获取所有的响应头 response.getheaders('Content-Type')获取一个响应头信息
    response.status #状态码

### 处理get请求的参数
    dict = {
        'a'='a'
    }
    urllib.parse.urlencode(query=dict, encoding='utf-8')处理get请求的参数
### urllib.request.Request() 构建一个请求对象
    url
    data
    headers={}
    origin_req_host
    method
