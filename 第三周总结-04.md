---
title: 12、第三周总结-04
tags: 
notebook: 第六个月总结笔记
---


# 第四天，scrapy框架2
    css '::text' css获取文本， 
        'a::attr(href)'获取到属性
        extract_first('')获取到第一个值
        
    xpth('.').re() 可以这么使用正则

    scrapy.Request(url, callback=回调函数， meta={})
        取值response.meta['键名']
    

### pipelines
### 如何自定义管道，如何激活管道，设置管道的优先级
    1、可以在__init__(sefl) 做参数的初始化
    2、def open_spider(self):  执行scrapy crawl 文件名 的时候运行一次
    3、def close_spider(self):  结束时运行
    4、process_item(self， item， spider)这个做数据的处理持久化

### 数据持久化
    mongodb mysql
    数据哭的信息一般都会在setting.py里面定义
    @classmethod
    def from_settings(cls, settings):
        xxx = settings['xxx']
    @classmethod
    def from_crawler(cls,crawler):
        xxx = crawler.settings['xxx']
        return cls(xxx)



### 插入MySQL数据库
    def insert_sql(dict):
        insert_sql = """
        INSERT INTO table(%s) VALUES(%s)
        """ % (','.jonin([key for key,value in dict.items()]),','.jonin(['%s' for key,value in dict.items()])

        parmas = [value for key,value in dict.items()]
